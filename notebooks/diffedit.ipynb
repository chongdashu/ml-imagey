{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install fastai\n",
    "%pip install accelerate\n",
    "%pip install transformers diffusers ftfy\n",
    "%pip install torch\n",
    "%pip install opencv-python\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffusers import AutoencoderKL, LMSDiscreteScheduler, StableDiffusionInpaintPipeline, UNet2DConditionModel\n",
    "from fastai.basics import show_image, show_images\n",
    "from fastcore.all import concat\n",
    "from fastdownload import FastDownload\n",
    "from huggingface_hub import notebook_login\n",
    "from PIL import Image\n",
    "from torch import autocast\n",
    "from torchvision import transforms as tfms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate\n",
    "path = Path.home() / \".cache\" / \"huggingface\" / \"token\"\n",
    "if not path.exists():\n",
    "    notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Load the autoencoder model which will be used to decode the latents into image space.\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "\n",
    "# Load the tokenizer and text encoder to tokenize and encode the text.\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# The UNet model for generating the latents.\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n",
    "\n",
    "# The noise scheduler\n",
    "# hyper parameters match those used during training the model\n",
    "scheduler = LMSDiscreteScheduler(\n",
    "    beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000\n",
    ")\n",
    "\n",
    "# To the GPU we go!\n",
    "vae = vae.to(torch_device)\n",
    "text_encoder = text_encoder.to(torch_device)\n",
    "unet = unet.to(torch_device)\n",
    "vae_magic = 0.18215  # vae model trained with a scale term to get closer to unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2latent(im):\n",
    "    im = tfms.ToTensor()(im).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        latent = vae.encode(im.to(torch_device) * 2 - 1)\n",
    "    latent = latent.latent_dist.sample() * vae_magic\n",
    "    return latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_latent(latents):\n",
    "    with torch.no_grad():\n",
    "        return vae.decode(latents / vae_magic).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latents2images(latents):\n",
    "    latents = latents / vae_magic\n",
    "    with torch.no_grad():\n",
    "        imgs = vae.decode(latents).sample\n",
    "    imgs = (imgs / 2 + 0.5).clamp(0, 1)\n",
    "    imgs = imgs.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    imgs = (imgs * 255).round().astype(\"uint8\")\n",
    "    imgs = [Image.fromarray(i) for i in imgs]\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_for_prompt(prompt):\n",
    "    max_length = tokenizer.model_max_length\n",
    "    tokens = tokenizer([prompt], padding=\"max_length\", max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        embeddings = text_encoder(tokens.input_ids.to(torch_device))[0]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noise_pred(prompts, im_latents, seed=32, g=0.15):\n",
    "    height = 512  # default height of Stable Diffusion\n",
    "    width = 512  # default width of Stable Diffusion\n",
    "    num_inference_steps = 30  # Number of denoising steps\n",
    "    generator = torch.manual_seed(seed)  # Seed generator to create the inital latent noise\n",
    "\n",
    "    uncond = get_embedding_for_prompt(\"\")\n",
    "    text = get_embedding_for_prompt(prompts)\n",
    "    text_embeddings = torch.cat([uncond, text])\n",
    "\n",
    "    # Prep Scheduler\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    # Prep latents\n",
    "    if im_latents != None:\n",
    "        # img2img\n",
    "        # start_step = 10\n",
    "        start_step = int(num_inference_steps * 0.5)\n",
    "        timesteps = torch.tensor([scheduler.timesteps[-start_step]], device=torch_device)\n",
    "        noise = torch.randn_like(im_latents)\n",
    "        latents = scheduler.add_noise(im_latents, noise, timesteps=timesteps)\n",
    "        latents = latents.to(torch_device).float()\n",
    "    else:\n",
    "        # just text prompts\n",
    "        start_step = -1  # disable branching below\n",
    "        latents = torch.randn((1, unet.in_channels, height // 8, width // 8))  # ,generator=generator)\n",
    "        latents = latents.to(torch_device)\n",
    "        latents = latents * scheduler.init_noise_sigma  # scale to initial amount of noise for t0\n",
    "\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "    latent_model_input = scheduler.scale_model_input(latent_model_input, timesteps)\n",
    "    with torch.no_grad():\n",
    "        u, t = unet(latent_model_input, timesteps, encoder_hidden_states=text_embeddings).sample.chunk(2)\n",
    "    pred_nonscaled = u + g * (t - u) / torch.norm(t - u) * torch.norm(u)\n",
    "    pred = pred_nonscaled * torch.norm(u) / torch.norm(pred_nonscaled)\n",
    "    return scheduler.step(pred, timesteps, latents).pred_original_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_from_embedding(text_embeddings, im_latents, mask=None, seed=None, guidance_scale=0.15):\n",
    "    height = 512  # default height of Stable Diffusion\n",
    "    width = 512  # default width of Stable Diffusion\n",
    "    num_inference_steps = 30  # Number of denoising steps\n",
    "    if seed is None:\n",
    "        seed = torch.seed()\n",
    "    generator = torch.manual_seed(seed)  # Seed generator to create the inital latent noise\n",
    "\n",
    "    uncond = get_embedding_for_prompt(\"\")\n",
    "    text_embeddings = torch.cat([uncond, text_embeddings])\n",
    "\n",
    "    # Prep Scheduler\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    # Prep latents\n",
    "\n",
    "    if im_latents != None:\n",
    "        # img2img\n",
    "        start_step = 10\n",
    "        noise = torch.randn_like(im_latents)\n",
    "        latents = scheduler.add_noise(im_latents, noise, timesteps=torch.tensor([scheduler.timesteps[start_step]]))\n",
    "        latents = latents.to(torch_device).float()\n",
    "    else:\n",
    "        # just text prompts\n",
    "        start_step = -1  # disable branching below\n",
    "        latents = torch.randn((1, unet.in_channels, height // 8, width // 8))  # ,generator=generator)\n",
    "        latents = latents.to(torch_device)\n",
    "        latents = latents * scheduler.init_noise_sigma  # scale to initial amount of noise for t0\n",
    "\n",
    "    noisy_latent = latents.clone()\n",
    "    # Loop\n",
    "    noise_pred = None\n",
    "    for i, tm in tqdm(\n",
    "        enumerate(scheduler.timesteps), total=num_inference_steps, desc=\"Generating Masked Image for Prompt\"\n",
    "    ):\n",
    "        if i > start_step:\n",
    "            # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "            latent_model_input = scheduler.scale_model_input(latent_model_input, tm)\n",
    "\n",
    "            # predict the noise residual\n",
    "            with torch.no_grad():\n",
    "                noise_pred = unet(latent_model_input, tm, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
    "\n",
    "            # perform guidance\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "\n",
    "            u = noise_pred_uncond\n",
    "            g = guidance_scale\n",
    "            t = noise_pred_text\n",
    "\n",
    "            if g > 0:\n",
    "                pred_nonscaled = u + g * (t - u) / torch.norm(t - u) * torch.norm(u)\n",
    "                pred = pred_nonscaled * torch.norm(u) / torch.norm(pred_nonscaled)\n",
    "            else:\n",
    "                pred = u\n",
    "\n",
    "            noise_pred = pred\n",
    "\n",
    "            # compute the previous noisy sample x_t -> x_t-1\n",
    "            latents = scheduler.step(noise_pred, tm, latents).prev_sample\n",
    "            if mask is not None:\n",
    "                latents = latents * mask + im_latents * (1.0 - mask)\n",
    "\n",
    "    noise_pred = noisy_latent - latents\n",
    "    return latents2images(latents)[0], noise_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2latentmask(im):\n",
    "    im = tfms.ToTensor()(im).permute(1, 2, 0)\n",
    "    m = im.mean(-1)  # convert to grayscale\n",
    "    m = (m > 0.5).float()  # binarize to 0.0 or 1.0\n",
    "    m = cv2.resize(m.cpu().numpy(), (64, 64), interpolation=cv2.INTER_NEAREST)\n",
    "    m = torch.tensor(m).to(torch_device)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-imagey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
